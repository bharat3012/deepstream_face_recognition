{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5134da5-b99d-4cd4-9b3d-5c5031a3425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e92f8d-95df-49c7-84ff-94a8317c4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and Initialize\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import configparser\n",
    "import numpy as np\n",
    "import gi\n",
    "gi.require_version('Gst', '1.0')\n",
    "gi.require_version('GstRtspServer', '1.0')\n",
    "from gi.repository import GObject, Gst, GstRtspServer#\n",
    "from common.is_aarch_64 import is_aarch64\n",
    "from common.bus_call import bus_call\n",
    "from common.FPS import GETFPS\n",
    "\n",
    "from facenet_utils import load_dataset, normalize_vectors, predict_using_classifier\n",
    "\n",
    "import ctypes\n",
    "import pyds\n",
    "import time\n",
    "\n",
    "fps_stream=None\n",
    "face_counter= []\n",
    "\n",
    "PGIE_CLASS_ID_PERSON = 2\n",
    "SGIE_CLASS_ID_FACE = 0\n",
    "\n",
    "PRIMARY_DETECTOR_UID = 1\n",
    "SECONDARY_DETECTOR_UID = 2\n",
    "\n",
    "#face embeddings\n",
    "DATASET_PATH = 'embeddings/nic-faces-dataset-embeddings_v1.npz'\n",
    "\n",
    "# Defining the input output video file \n",
    "stream_path  = '/opt/nvidia/deepstream/deepstream-5.1/sources/deepstream_python_apps/apps/face_recognition/s_vid.264'\n",
    "output_path = \"ds_out.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a1b4fb",
   "metadata": {},
   "source": [
    "#### Initialise GStreamer and Create an Empty Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07266587",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps_stream = GETFPS(0)\n",
    "print(fps_stream)\n",
    "# Standard GStreamer initialization\n",
    "GObject.threads_init()\n",
    "Gst.init(None)\n",
    "\n",
    "# Create gstreamer elements\n",
    "# Create Pipeline element that will form a connection of other elements\n",
    "print(\"Creating Pipeline \\n \")\n",
    "pipeline = Gst.Pipeline()\n",
    "\n",
    "if not pipeline:\n",
    "    sys.stderr.write(\" Unable to create Pipeline \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54decd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make Element or Print Error and any other detail\n",
    "def make_elm_or_print_err(factoryname, name, printedname, detail=\"\"):\n",
    "  print(\"Creating\", printedname)\n",
    "  elm = Gst.ElementFactory.make(factoryname, name)\n",
    "  if not elm:\n",
    "     sys.stderr.write(\"Unable to create \" + printedname + \" \\n\")\n",
    "  if detail:\n",
    "     sys.stderr.write(detail)\n",
    "  return elm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55702c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source element for reading from the file\n",
    "source = make_elm_or_print_err(\"filesrc\", \"file-source\",\"Source\")\n",
    "# Since the data format in the input file is elementary h264 stream,\n",
    "# we need a h264parsermake_elm_or_print_err(\"filesrc\", \"file-source\",\"Source\")\n",
    "h264parser = make_elm_or_print_err(\"h264parse\", \"h264-parser\", \"h264 parser\")\n",
    "# Use nvdec_h264 for hardware accelerated decode on GPU\n",
    "decoder = make_elm_or_print_err(\"nvv4l2decoder\", \"nvv4l2-decoder\", \"Decoder\")\n",
    "# Create nvstreammux instance to form batches from one or more sources.\n",
    "streammux = make_elm_or_print_err(\"nvstreammux\", \"Stream-muxer\", \"NvStreamMux\")\n",
    "# Use nvinfer to run inferencing on decoder's output,\n",
    "# behaviour of inferencing is set through config file\n",
    "#Detector\n",
    "face_detector = make_elm_or_print_err(\"nvinfer\", \"primary-inference face detector\", \"face_detector\")\n",
    "#Classifier\n",
    "face_classifier = make_elm_or_print_err(\"nvinfer\", \"secondary-inference face_classifier\", \"face_classifier\")\n",
    "# Use convertor to convert from NV12 to RGBA as required by nvosd\n",
    "nvvidconv = make_elm_or_print_err(\"nvvideoconvert\", \"convertor\", \"nvvidconv\")\n",
    "# Create OSD to draw on the converted RGBA buffer\n",
    "nvosd = make_elm_or_print_err(\"nvdsosd\", \"onscreendisplay\", \"nvosd\" )\n",
    "# Finally encode and save the osd output\n",
    "queue = make_elm_or_print_err(\"queue\", \"queue\", \"Queue\")\n",
    "# Use convertor to convert from NV12 to RGBA as required by nvosd\n",
    "nvvidconv2 = make_elm_or_print_err(\"nvvideoconvert\", \"convertor2\",\"nvvidconv2\")\n",
    "# Place an encoder instead of OSD to save as video file\n",
    "encoder = make_elm_or_print_err(\"avenc_mpeg4\", \"encoder\", \"Encoder\")\n",
    "# Parse output from Encoder \n",
    "codeparser = make_elm_or_print_err(\"mpeg4videoparse\", \"mpeg4-parser\", 'Code Parser')\n",
    "# Create a container\n",
    "container = make_elm_or_print_err(\"qtmux\", \"qtmux\", \"Container\")\n",
    "# Create Sink for storing the output \n",
    "sink = make_elm_or_print_err(\"filesink\", \"filesink\", \"Sink\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd4a58c",
   "metadata": {},
   "source": [
    "Now that we have created the elements ,we can now set various properties for out pipeline at this point. \n",
    "\n",
    "For the sgie1 , sgie2 and sgie3 , we use `operate-on-gie-id` and `operate-on-class-ids`, we configured the pipeline to only evaluate the make, model, and color of objects classified as cars by our primary model.\n",
    "\n",
    "You can access the configuration files here : [face_detector](detector_config.txt) , [face_classifier](classifier_config.txt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eea50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Set properties for the Elements ############\n",
    "print(\"Playing file %s\", stream_path)\n",
    "# Set Input File Name \n",
    "source.set_property('location', stream_path)\n",
    "# Set Input Width , Height and Batch Size \n",
    "streammux.set_property('width', 1920)\n",
    "streammux.set_property('height', 1080)\n",
    "streammux.set_property('batch-size', 1)\n",
    "\n",
    "# Set Timeout in microseconds to wait after the first buffer is  \n",
    "# available to push the batch even if a complete batch is not formed.\n",
    "streammux.set_property('batched-push-timeout', 4000000)\n",
    "\n",
    "# Set Congifuration file for nvinfer \n",
    "face_detector.set_property('config-file-path', \"detector_config.txt\")\n",
    "face_classifier.set_property('config-file-path', \"classifier_config.txt\")\n",
    "\n",
    "# Set Encoder bitrate for output video\n",
    "encoder.set_property(\"bitrate\", 2000000)\n",
    "sink.set_property('location', output_path)\n",
    "sink.set_property('async', False)\n",
    "sink.set_property('sync', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6646a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding elements to Pipeline \\n\")\n",
    "pipeline.add(source)\n",
    "pipeline.add(h264parser)\n",
    "pipeline.add(decoder)\n",
    "pipeline.add(streammux)\n",
    "pipeline.add(face_detector)\n",
    "pipeline.add(face_classifier)\n",
    "pipeline.add(nvvidconv)\n",
    "pipeline.add(nvosd)\n",
    "pipeline.add(queue)\n",
    "pipeline.add(nvvidconv2)\n",
    "pipeline.add(encoder)\n",
    "pipeline.add(codeparser)\n",
    "pipeline.add(container)\n",
    "pipeline.add(sink)\n",
    "\n",
    "if is_aarch64():\n",
    "    pipeline.add(transform)\n",
    "\n",
    "# we link the elements together\n",
    "# file-source -> h264-parser -> nvh264-decoder ->\n",
    "# nvinfer -> nvvidconv -> nvosd -> video-renderer\n",
    "print(\"Linking elements in the Pipeline \\n\")\n",
    "source.link(h264parser)\n",
    "h264parser.link(decoder)\n",
    "\n",
    "sinkpad = streammux.get_request_pad(\"sink_0\")\n",
    "if not sinkpad:\n",
    "    sys.stderr.write(\" Unable to get the sink pad of streammux \\n\")\n",
    "srcpad = decoder.get_static_pad(\"src\")\n",
    "if not srcpad:\n",
    "    sys.stderr.write(\" Unable to get source pad of decoder \\n\")\n",
    "\n",
    "srcpad.link(sinkpad)\n",
    "streammux.link(face_detector)\n",
    "face_detector.link(face_classifier)\n",
    "face_classifier.link(nvvidconv)\n",
    "nvvidconv.link(nvosd)\n",
    "\n",
    "nvosd.link(queue)\n",
    "queue.link(nvvidconv2)\n",
    "nvvidconv2.link(encoder)\n",
    "encoder.link(codeparser)\n",
    "codeparser.link(container)\n",
    "container.link(sink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc0cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and event loop and feed gstreamer bus mesages to it\n",
    "loop = GObject.MainLoop()\n",
    "\n",
    "bus = pipeline.get_bus()\n",
    "bus.add_signal_watch()\n",
    "bus.connect (\"message\", bus_call, loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7548ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def osd_sink_pad_buffer_probe(pad,info,u_data):\n",
    "    global fps_stream, face_counter\n",
    "    frame_number=0\n",
    "    #Intiallizing object counter with 0.\n",
    "    person_count = 0\n",
    "    face_count = 0\n",
    "    num_rects=0\n",
    "\n",
    "    gst_buffer = info.get_buffer()\n",
    "    if not gst_buffer:\n",
    "        print(\"Unable to get GstBuffer \")\n",
    "        return\n",
    "\n",
    "    # Retrieve batch metadata from the gst_buffer\n",
    "    # Note that pyds.gst_buffer_get_nvds_batch_meta() expects the\n",
    "    # C address of gst_buffer as input, which is obtained with hash(gst_buffer)\n",
    "    batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer))\n",
    "    l_frame = batch_meta.frame_meta_list\n",
    "    while l_frame is not None:\n",
    "        try:\n",
    "            # Note that l_frame.data needs a cast to pyds.NvDsFrameMeta\n",
    "            # The casting is done by pyds.glist_get_nvds_frame_meta()\n",
    "            # The casting also keeps ownership of the underlying memory\n",
    "            # in the C code, so the Python garbage collector will leave\n",
    "            # it alone.\n",
    "            frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        frame_number=frame_meta.frame_num\n",
    "        num_rects = frame_meta.num_obj_meta\n",
    "        l_obj=frame_meta.obj_meta_list\n",
    "\n",
    "        while l_obj is not None:\n",
    "            try:\n",
    "                # Casting l_obj.data to pyds.NvDsObjectMeta\n",
    "                obj_meta=pyds.NvDsObjectMeta.cast(l_obj.data)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            if obj_meta.unique_component_id == PRIMARY_DETECTOR_UID:\n",
    "                if obj_meta.class_id == PGIE_CLASS_ID_PERSON:\n",
    "                   person_count += 1\n",
    "                \n",
    "            if obj_meta.unique_component_id == SECONDARY_DETECTOR_UID:\n",
    "                if obj_meta.class_id == SGIE_CLASS_ID_FACE:\n",
    "                   face_count += 1\n",
    "            try:\n",
    "                l_obj=l_obj.next\n",
    "            except StopIteration:\n",
    "                break\n",
    "        fps_stream.get_fps()\n",
    "        # Acquiring a display meta object. The memory ownership remains in\n",
    "        # the C code so downstream plugins can still access it. Otherwise\n",
    "        # the garbage collector will claim it when this probe function exits.\n",
    "        display_meta=pyds.nvds_acquire_display_meta_from_pool(batch_meta)\n",
    "        display_meta.num_labels = 1\n",
    "        py_nvosd_text_params = display_meta.text_params[0]\n",
    "        # Setting display text to be shown on screen\n",
    "        # Note that the pyds module allocates a buffer for the string, and the\n",
    "        # memory will not be claimed by the garbage collector.\n",
    "        # Reading the display_text field here will return the C address of the\n",
    "        # allocated string. Use pyds.get_string() to get the string content.\n",
    "        py_nvosd_text_params.display_text = \"Frame Number={} Number of Objects={} Face Count={}\".format(frame_number, num_rects, num_rects)\n",
    "        face_counter.append(person_count)\n",
    "\n",
    "        # Now set the offsets where the string should appear\n",
    "        py_nvosd_text_params.x_offset = 10\n",
    "        py_nvosd_text_params.y_offset = 12\n",
    "\n",
    "        # Font , font-color and font-size\n",
    "        py_nvosd_text_params.font_params.font_name = \"Serif\"\n",
    "        py_nvosd_text_params.font_params.font_size = 10\n",
    "        # set(red, green, blue, alpha); set to White\n",
    "        py_nvosd_text_params.font_params.font_color.set(1.0, 1.0, 1.0, 1.0)\n",
    "\n",
    "        # Text background color\n",
    "        py_nvosd_text_params.set_bg_clr = 1\n",
    "        # set(red, green, blue, alpha); set to Black\n",
    "        py_nvosd_text_params.text_bg_clr.set(0.0, 0.0, 0.0, 1.0)\n",
    "        # Using pyds.get_string() to get display_text as string\n",
    "        print(pyds.get_string(py_nvosd_text_params.display_text))\n",
    "        pyds.nvds_add_display_meta_to_frame(frame_meta, display_meta)\n",
    "        try:\n",
    "            l_frame=l_frame.next\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return Gst.PadProbeReturn.OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d93066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets add probe to get informed of the meta data generated, we add probe to\n",
    "# the sink pad of the osd element, since by that time, the buffer would have\n",
    "# had got all the metadata.\n",
    "osdsinkpad = nvosd.get_static_pad(\"sink\")\n",
    "if not osdsinkpad:\n",
    "    sys.stderr.write(\" Unable to get sink pad of nvosd \\n\")\n",
    "\n",
    "osdsinkpad.add_probe(Gst.PadProbeType.BUFFER, osd_sink_pad_buffer_probe, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d0115",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_embeddings, labels = load_dataset(DATASET_PATH)\n",
    "\n",
    "def sgie_sink_pad_buffer_probe(pad,info,u_data):\n",
    "\n",
    "    frame_number=0\n",
    "\n",
    "    num_rects=0\n",
    "    gst_buffer = info.get_buffer()\n",
    "    if not gst_buffer:\n",
    "        print(\"Unable to get GstBuffer \")\n",
    "        return\n",
    "\n",
    "    # Retrieve batch metadata from the gst_buffer\n",
    "    # Note that pyds.gst_buffer_get_nvds_batch_meta() expects the\n",
    "    # C address of gst_buffer as input, which is obtained with hash(gst_buffer)\n",
    "    batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer))\n",
    "    #print(\"BM:\", batch_meta)\n",
    "    l_frame = batch_meta.frame_meta_list\n",
    "    #print(\"l_frame:\", l_frame)\n",
    "    while l_frame is not None:\n",
    "        try:\n",
    "            # Note that l_frame.data needs a cast to pyds.NvDsFrameMeta\n",
    "            # The casting is done by pyds.NvDsFrameMeta.cast()\n",
    "            # The casting also keeps ownership of the underlying memory\n",
    "            # in the C code, so the Python garbage collector will leave\n",
    "            # it alone.\n",
    "            frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        frame_number=frame_meta.frame_num\n",
    "        num_rects = frame_meta.num_obj_meta\n",
    "\n",
    "        l_obj=frame_meta.obj_meta_list\n",
    "        #print(\"l_obj:\", l_obj)\n",
    "        \n",
    "        while l_obj is not None:\n",
    "            try:\n",
    "                # Casting l_obj.data to pyds.NvDsObjectMeta\n",
    "                obj_meta=pyds.NvDsObjectMeta.cast(l_obj.data)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            #print(\"obj_meta:\",obj_meta)  \n",
    "            l_user = obj_meta.obj_user_meta_list\n",
    "            while l_user is not None:\n",
    "\n",
    "                try:\n",
    "                    # Casting l_user.data to pyds.NvDsUserMeta\n",
    "                    user_meta=pyds.NvDsUserMeta.cast(l_user.data)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "\n",
    "                if (\n",
    "                    user_meta.base_meta.meta_type\n",
    "                    != pyds.NvDsMetaType.NVDSINFER_TENSOR_OUTPUT_META\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                # Converting to tensor metadata\n",
    "                # Casting user_meta.user_meta_data to NvDsInferTensorMeta\n",
    "                tensor_meta = pyds.NvDsInferTensorMeta.cast(user_meta.user_meta_data)\n",
    "\n",
    "                # Get output layer as NvDsInferLayerInfo\n",
    "                layer = pyds.get_nvds_LayerInfo(tensor_meta, 0)\n",
    "\n",
    "                # Convert NvDsInferLayerInfo buffer to numpy array\n",
    "                ptr = ctypes.cast(pyds.get_ptr(layer.buffer), ctypes.POINTER(ctypes.c_float))\n",
    "                v = np.ctypeslib.as_array(ptr, shape=(128,))\n",
    "\n",
    "                # Pridict face neme\n",
    "                yhat = v.reshape((1,-1))\n",
    "                face_to_predict_embedding = normalize_vectors(yhat)\n",
    "                result = predict_using_classifier(faces_embeddings, labels, face_to_predict_embedding)\n",
    "                result =  (str(result).title())\n",
    "                print('Predicted name: %s' % result)\n",
    "                # Generate classifer metadata and attach to obj_meta\n",
    "                # Get NvDsClassifierMeta object\n",
    "                classifier_meta = pyds.nvds_acquire_classifier_meta_from_pool(batch_meta)\n",
    "\n",
    "                # Pobulate classifier_meta data with pridction result\n",
    "                classifier_meta.unique_component_id = tensor_meta.unique_id\n",
    "\n",
    "\n",
    "                label_info = pyds.nvds_acquire_label_info_meta_from_pool(batch_meta)\n",
    "\n",
    "\n",
    "                label_info.result_prob = 0\n",
    "                label_info.result_class_id = 0\n",
    "\n",
    "                pyds.nvds_add_label_info_meta_to_classifier(classifier_meta, label_info)\n",
    "                pyds.nvds_add_classifier_meta_to_object(obj_meta, classifier_meta)\n",
    "\n",
    "                display_text = pyds.get_string(obj_meta.text_params.display_text)\n",
    "                obj_meta.text_params.display_text = f'{display_text} {result}'\n",
    "\n",
    "                try:\n",
    "                    l_user = l_user.next\n",
    "                except StopIteration:\n",
    "                    break\n",
    "\n",
    "            try:\n",
    "                l_obj=l_obj.next\n",
    "            except StopIteration:\n",
    "                break\n",
    "        try:\n",
    "            l_frame=l_frame.next\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return Gst.PadProbeReturn.OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b34ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vidconvsinkpad = nvvidconv.get_static_pad(\"sink\")\n",
    "\n",
    "if not vidconvsinkpad:\n",
    "    sys.stderr.write(\" Unable to get sink pad of nvvidconv \\n\")\n",
    "\n",
    "vidconvsinkpad.add_probe(Gst.PadProbeType.BUFFER, sgie_sink_pad_buffer_probe, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e32eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start play back and listen to events\n",
    "print(\"Starting pipeline \\n\")\n",
    "start_time = time.time()\n",
    "pipeline.set_state(Gst.State.PLAYING)\n",
    "try:\n",
    "    loop.run()\n",
    "except:\n",
    "    pass\n",
    "# cleanup\n",
    "pipeline.set_state(Gst.State.NULL)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert video profile to be compatible with Jupyter notebook\n",
    "!ffmpeg -loglevel panic -y -an -i ds_out.mp4 -vcodec libx264 -pix_fmt yuv420p -profile:v baseline -level 3 output.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51301ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Output\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    " <video width=\"640\" height=\"480\" controls>\n",
    " <source src=\"output.mp4\"\n",
    " </video>\n",
    "\"\"\".format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26310bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
